<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Data-Driven Loss Functions for Inference-Time Optimization in Text-to-Image Generation - A novel framework for improving spatial reasoning in text-to-image models.">
  <meta name="keywords" content="Text-to-Image Generation, Spatial Reasoning, Diffusion Models, Cross-Attention Maps, Computer Vision">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Data-Driven Loss Functions for Inference-Time Optimization in Text-to-Image Generation</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <!-- <link rel="stylesheet" href="./static/css/bulma-slider.min.css"> -->
  <!-- <link rel="stylesheet" href="./static/css/fontawesome.all.min.css"> -->
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <style>
    .vertical-divider {
      border-left: 2px dashed #999;
      height: 450px;
      margin: 0 auto;
      width: 2px;
    }

    .vertical-divider-architecture {
      border-left: 2px dashed #999;
      height: 480px;
      margin: 0 auto;
      width: 2px;
    }
    
    .teaser-figure .columns {
      align-items: center;
    }
    
    /* Custom Carousel Styles */
    .image-carousel {
      position: relative;
      margin: 2rem 0;
      max-width: 100%;
      overflow: visible;
    }
    
    .carousel-container {
      position: relative;
      width: 100%;
    }
    
    .carousel-track {
      display: flex;
      transition: transform 0.5s ease-in-out;
    }
    
    .carousel-item {
      min-width: 100%;
      display: none;
    }

    .carousel-item img {
      width: 100%;
      border-radius: 8px;
      /*border: 10px solid white;*/
      border: 0px solid white;
      box-shadow: 0 4px 8px rgba(0,0,0,0.15);
    }
    
    .carousel-item.active {
      display: block;
    }

    .examples-section {
      padding-bottom: 1rem;  /* shrink bottom padding */
    }

    /* Reduce vertical spacing between Examples and Abstract */
    .examples-section .column {
      max-width: 100%;  /* don’t let Bulma shrink it */
    }
    
    .examples-row {
      display: flex;
      justify-content: center;
      align-items: center;
      gap: 1rem;
      padding: 0 0.5rem;
    }
    
    .example-item {
      text-align: center;
      flex: 1;
      max-width: 600px;
    }
    
    .abstract-section {
      padding-top: 1rem;     /* shrink top padding */
    }

    .teaser-section {
      padding-top: 0rem;     /* shrink top padding */
      padding-bottom: 1rem;  /* shrink bottom padding */
    }
    
    .example-title {
      font-size: 1.1rem;
      margin-bottom: 1rem;
      color: #363636;
      font-weight: 600;
    }
    
    .carousel-nav{
      position: absolute; 
      top: calc(50% - 10px); 
      transform: translateY(-50%);
      width: 40px; 
      height: 40px; 
      border-radius: 50%;
      background: rgba(255,255,255,.95);
      border: 2px solid rgba(255,255,255,.8);
      display: flex; 
      align-items: center; 
      justify-content: center; /* centers icon */
      color: #4c4c4c; 
      cursor: pointer; 
      box-shadow: 0 4px 8px rgba(0,0,0,.3);
      transition: all .3s ease; 
      z-index: 20;
    }
    .carousel-nav i{ 
      font-size: 22px; 
      line-height: 1; 
      pointer-events:none; 
    }
    .carousel-nav:hover{ 
      background:#fff; 
      box-shadow:0 4px 8px rgba(0,0,0,.4); 
      transform:translateY(-50%) scale(1.1);
    }
    
    .carousel-nav.prev {
      left: -30px;
    }
    
    .carousel-nav.next {
      right: -30px;
    }

    
    /* Mobile landscape / small tablets: 720px–1023px */
    @media (min-width: 720px) and (max-width: 1023.98px) {
      .carousel-nav.prev { left: 20px; }
      .carousel-nav.next { right: 20px; }
    }

    /* Desktop: >=1024px */
    @media (min-width: 1024px) {
      .carousel-nav.prev { left: 135px; }
      .carousel-nav.next { right: 135px; }
    }

    .carousel-dots {
      display: flex;
      justify-content: center;
      align-items: center;
      margin-top: 0.5rem;
      gap: 8px;
    }
    
    .dot {
      width: 8px;
      height: 8px;
      border-radius: 50%;
      background: #ccc;
      cursor: pointer;
      transition: background 0.3s ease;
    }
    
    .dot.active {
      width: 12px;
      height: 12px;
      background: #4c4c4c;
    }
    
    .dot:hover {
      background: #999;
    }

    .highlight-title {
      font-weight: 900;
      color: #932842;
    }
    
  </style>

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  
</head>
<body>

<!-- Hero Section -->
<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Data-Driven Loss Functions for Inference-Time Optimization in Text-to-Image Generation</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=xfyZ4HYAAAAJ&hl=en">Sapir Esther Yiflach</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://research.nvidia.com/person/yuval-atzmon">Yuval Atzmon</a><sup>2</sup>,</span>
            <span class="author-block">
              <a href="https://chechiklab.biu.ac.il/~gal/">Gal Chechik</a><sup>1,2</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Bar-Ilan University,</span>
            <span class="author-block"><sup>2</sup>NVIDIA</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <span class="link-block">
                <a href="https://arxiv.org/abs/2509.02295"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- <span class="link-block">
                <a href="#"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span> -->
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Teaser Images Section -->
<section class="section teaser-section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <div class="teaser-figure">
          <!-- <h2 class="subtitle has-text-centered" style="margin-top: 2rem;"> -->
          <h2 class="subtitle has-text-centered">
            We present <strong><span class="dnerf highlight-title"">Learn-to-Steer</span></strong>, a test-time optimization method that 
            generates correct spatial arrangements between objects in text-to-image generation. 
            Rather than relying on handcrafted losses, our approach learns directly from attention maps to understand 
            spatial relationships specified in prompts. Learn-to-Steer works across different diffusion architectures, 
            including MMDiT. Finally, despite learning from single relations, it generalizes to combining several spatial 
            relations within a single generated image.

            <!-- option 2 -->
            <!-- <span class="dnerf">Learn-to-Steer</span> generates the correct spatial arrangement between objects specified in the input prompt by learning a loss directly from attention maps. -->
  
          </h2>
          <!-- Desktop layout with divider -->
          <div class="columns is-centered is-vcentered is-hidden-mobile">
            <!-- Left image -->
            <div class="column is-5">
              <img src="./static/images/fig1a_NO_letters__cropped.png" alt="Learn-to-Steer Spatial Orientations" style="width: 100%; height: auto;">
              <p class="has-text-centered" style="margin-top: 0.5rem;"><strong>Spatial Orientations</strong></p>
            </div>

            <!-- Divider -->
            <div class="column is-narrow">
              <div class="vertical-divider"></div>
            </div>

            <!-- Right image -->
            <div class="column is-5">
              <img src="./static/images/fig1bc_NO_letters__cropped.png" alt="Learn-to-Steer Multiple Relations" style="width: 100%; height: auto;">
              <p class="has-text-centered" style="margin-top: 0.5rem;"><strong>Multiple Relations</strong></p>
            </div>
          </div>

          <!-- Mobile layout without divider -->
          <div class="is-hidden-tablet">
            <div class="has-text-centered" style="margin-bottom: 2rem;">
              <img src="./static/images/fig1a_NO_letters__cropped.png" alt="Learn-to-Steer Spatial Orientations" style="width: 100%; max-width: 400px; height: auto;">
              <p class="has-text-centered" style="margin-top: 0.5rem;"><strong>Spatial Orientations</strong></p>
            </div>
            
            <div class="has-text-centered">
              <img src="./static/images/fig1bc_NO_letters__cropped.png" alt="Learn-to-Steer Multiple Relations" style="width: 100%; max-width: 400px; height: auto;">
              <p class="has-text-centered" style="margin-top: 0.5rem;"><strong>Multiple Relations</strong></p>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Examples Section -->
<section class="section has-background-light examples-section">
  <!-- <div class="container is-fluid"> -->
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
      <!-- <div class="column"> -->
        <h2 class="title is-3">Examples of Text-to-Image Generation with Learn-to-Steer</h2>

        <p>
          Uncurated results from Learn-to-Steer. The approach easily generates spatially aligned images. 
          <!-- Green boxes mark spatially aligned images (per the <em>GenEval</em> benchmark), while red boxes mark misaligned ones. -->
        </p>
        <p>
          Green boxes mark aligned images, while misaligned ones are in red (per the <em>GenEval</em> benchmark).
        </p>
        
        <div class="image-carousel">
          <div class="carousel-container">
            <div class="carousel-track">
              <div class="carousel-item active"><div class="examples-row"><div class="example-item">
                  <img src="./static/images/uncurated_split/row1_crop1.png" alt="Spatial Reasoning Example 1">
              </div></div></div>
        
              <div class="carousel-item"><div class="examples-row"><div class="example-item">
                <img src="./static/images/uncurated_split/row2_crop1.png" alt="Spatial Reasoning Example 2">
              </div></div></div>
        
              <div class="carousel-item"><div class="examples-row"><div class="example-item">
                <img src="./static/images/uncurated_split/row3_crop1.png" alt="Spatial Reasoning Example 3">
              </div></div></div>
        
              <div class="carousel-item"><div class="examples-row"><div class="example-item">
                <img src="./static/images/uncurated_split/row4_crop1.png" alt="Spatial Reasoning Example 4">
              </div></div></div>
        
              <div class="carousel-item"><div class="examples-row"><div class="example-item">
                <img src="./static/images/uncurated_split/row5_crop1.png" alt="Spatial Reasoning Example 5">
              </div></div></div>
        
              <div class="carousel-item"><div class="examples-row"><div class="example-item">
                <img src="./static/images/uncurated_split/row6_crop1.png" alt="Spatial Reasoning Example 6">
              </div></div></div>
        
              <div class="carousel-item"><div class="examples-row"><div class="example-item">
                <img src="./static/images/uncurated_split/row7_crop1.png" alt="Spatial Reasoning Example 7">
              </div></div></div>
            </div> <!-- /.carousel-track -->
          </div>   <!-- /.carousel-container -->
        
          <!-- ONE set of nav buttons -->
          <button class="carousel-nav prev" onclick="moveCarousel(-1)" aria-label="Previous">
            <i class="fas fa-chevron-left" aria-hidden="true"></i>
          </button>
          <button class="carousel-nav next" onclick="moveCarousel(1)" aria-label="Next">
            <i class="fas fa-chevron-right" aria-hidden="true"></i>
          </button>
        
          <!-- ONE dots row -->
          <div class="carousel-dots">
            <span class="dot active" onclick="goToSlide(0)"></span>
            <span class="dot" onclick="goToSlide(1)"></span>
            <span class="dot" onclick="goToSlide(2)"></span>
            <span class="dot" onclick="goToSlide(3)"></span>
            <span class="dot" onclick="goToSlide(4)"></span>
            <span class="dot" onclick="goToSlide(5)"></span>
            <span class="dot" onclick="goToSlide(6)"></span>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Abstract Section -->
<section class="section has-background-light abstract-section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Text-to-image diffusion models can generate stunning visuals, yet they often fail at tasks children find trivial—like placing a dog to the right of a teddy bear rather than to the left. When combinations get more unusual—a giraffe above an airplane—these failures become even more pronounced. Existing methods attempt to fix these spatial reasoning failures through model fine-tuning or test-time optimization with handcrafted losses that are suboptimal. Rather than imposing our assumptions about spatial encoding, we propose learning these objectives directly from the model's internal representations.
          </p>
          <p>
            We introduce <span class="dnerf">Learn-to-Steer</span>, a novel framework that learns data-driven objectives for test-time optimization rather than handcrafting them. Our key insight is to train a lightweight classifier that decodes spatial relationships from the diffusion model's cross-attention maps, then deploy this classifier as a learned loss function during inference. Training such classifiers poses a surprising challenge: they can take shortcuts by detecting linguistic traces rather than learning true spatial patterns. We solve this with a dual-inversion strategy that enforces geometric understanding.
          </p>
          <p>
            Our method dramatically improves spatial accuracy: from 0.20 to 0.61 on FLUX.1-dev and from 0.07 to 0.54 on SD2.1 across standard benchmarks. Moreover, our approach generalizes to multiple relations and significantly improves accuracy.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- How it Works Section: vertical divider -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">How does Learn-to-Steer work?</h2>
        
        <div class="architecture-figure">
          <!-- Desktop layout with divider -->
          <div class="columns is-centered is-vcentered is-hidden-mobile">
            <div class="column is-6">
              <img src="./static/images/training_pipeline.png" alt="Training Pipeline" style="width: 100%; height: auto;">
              <p class="has-text-centered" style="margin-top: 0.5rem;"><strong>Training Pipeline</strong></p>
              <p class="has-text-centered" style="margin-top: 0.5rem; font-size: 0.9rem; color: #666;">
                Given a spatially-aligned image, we perform dual image inversion (positive and negative prompts) to prevent relation leakage. During denoising, we extract the relevant attention maps and use them to train our classifier.
              </p>
            </div>

            <div class="column is-narrow">
              <div class="vertical-divider-architecture"></div>
            </div>

            <div class="column is-6">
              <img src="./static/images/inference_pipeline.png" alt="Inference Pipeline" style="width: 100%; height: auto;">
              <p class="has-text-centered" style="margin-top: 0.5rem;"><strong>Test-time Optimization Pipeline</strong></p>
              <p class="has-text-centered" style="margin-top: 0.5rem; font-size: 0.9rem; color: #666;">
                During inference, we extract the relevant cross-attention maps when denoising z_t and evaluate their relationship using our trained relation classifier. We then update the latent noise with backpropagation.
              </p>
            </div>
          </div>

          <!-- Mobile layout without divider -->
          <div class="is-hidden-tablet">
            <div class="has-text-centered" style="margin-bottom: 2rem;">
              <img src="./static/images/training_pipeline.png" alt="Training Pipeline" style="width: 100%; max-width: 500px; height: auto;">
              <p class="has-text-centered" style="margin-top: 0.5rem;"><strong>Training Pipeline</strong></p>
              <p class="has-text-centered" style="margin-top: 0.5rem; font-size: 0.9rem; color: #666;">
                Given a spatially-aligned image, we perform dual image inversion (positive and negative prompts) to prevent relation leakage. During denoising, we extract the relevant attention maps and use them to train our classifier.
              </p>
            </div>
            
            <div class="has-text-centered">
              <img src="./static/images/inference_pipeline.png" alt="Inference Pipeline" style="width: 100%; max-width: 500px; height: auto;">
              <p class="has-text-centered" style="margin-top: 0.5rem;"><strong>Test-time Optimization Pipeline</strong></p>
              <p class="has-text-centered" style="margin-top: 0.5rem; font-size: 0.9rem; color: #666;">
                During inference, we extract the relevant cross-attention maps when denoising z_t and evaluate their relationship using our trained relation classifier. We then update the latent noise with backpropagation.
              </p>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Qualitative Results Section -->
<section class="section has-background-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Qualitative Results</h2>
        <!-- <div class="content has-text-justified">
          <p>
            We evaluated Learn-to-Steer against handcrafted losses, fine-tuning approaches, and other test-time optimization methods. Our method successfully generates correct spatial relationships, while other methods struggle in some or all of the examples.
          </p>
        </div> -->
        
        <div class="columns is-centered">
          <div class="column">
            <img src="./static/images/COMBINED__qa_dev_geneval_rank_1_2__qa_SD_2.1_geneval_rank_1_2.png" alt="Comparison Results" style="width: 100%; height: auto; border: 12px solid white; border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.15);">
            <p class="has-text-centered">
              We evaluated Learn-to-Steer against handcrafted losses and other test-time optimization methods. Our method successfully generates correct spatial relationships, while other methods struggle in all of the examples.
            </p>
          </div>
        </div>

        <div class="columns is-centered" style="margin-top: 1rem;">
          <div class="column">
            <img src="./static/images/unified_qa__geneval_rank_1.png" alt="Unified Results" style="width: 100%; height: auto; border: 12px solid white; border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.15);">
            <p class="has-text-centered">
              Comparisons with all four base models.
            </p>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- BibTeX Section -->
<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <p>If you find our work useful, please cite our paper:</p>
    <pre><code>@article{yiflach2025datadriven,
  title={Data-Driven Loss Functions for Inference-Time Optimization in Text-to-Image Generation},
  author={Yiflach, Sapir Esther and Atzmon, Yuval and Chechik, Gal},
  journal={arXiv preprint arXiv:2509.02295},
  year={2025},
}</code></pre>
  </div>
</section>

<!-- Footer -->
<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link" href="#">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="#" class="external-link">
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

<script>
  // Custom Carousel Functionality
  let currentSlide = 0;
  const totalSlides = 7;

  function moveCarousel(direction) {
    currentSlide = (currentSlide + direction + totalSlides) % totalSlides;
    updateCarousel();
  }

  function goToSlide(slideIndex) {
    currentSlide = slideIndex;
    updateCarousel();
  }

  function updateCarousel() {
    const items = document.querySelectorAll('.carousel-item');
    const dots = document.querySelectorAll('.dot');

    // Hide all items
    items.forEach(item => item.classList.remove('active'));
    
    // Show current item
    items[currentSlide].classList.add('active');

    // Update dots
    dots.forEach((dot, index) => {
      dot.classList.toggle('active', index === currentSlide);
    });
  }

  // Initialize carousel
  document.addEventListener('DOMContentLoaded', function() {
    updateCarousel();
  });
</script>

</body>
</html>
